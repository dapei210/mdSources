### IO多路复用

当socket设置为非阻塞时，connect时可能会返回失败 -1，如果errno是EINPROGRESS,表示连接还在进行中，后面可以通过poll或者select来判断socket是否可写。socket可写时，可以通过getsockopt来查看connect的结果


### 客户端连接
TCP连接就是在客户端、服务端上的一对socket。它们都在各自的内核对象上记录了双方的IP对、端口对(即四元组)，通过这个在通信时找到对方。

可以看出，客户端的端口只是这个四元组里的一元而已，哪怕两条连接用的是同一端口号，只要客户端IP不一样，或者是端不一样，都不影响内核正确
寻找到对应的连接，也不会串线。 

所以，在客户端增加TCP最大并发能力的两个方法：1，为客户端配置多个IP； 2，连接到不同的服务端

这两个方法最好不用混用，因为使用多IP，客户端需要绑定，一旦绑定之后，内核建立连接的时候就不会选择使用过的端口了。bind函数会改变内核选择端口的策略


### 性能优化
网络请求优化
1. 尽量减少不必要的网络IO
2. 尽量合并网络请求
3. 调用者和被调用机器尽可能部署得近一些
4. 内网调用不要用外网域名

接受过程优化

1. 调整网卡RingBuffer大小

在整个网络协议栈中，RingBuffer扮演一个任务是收发中转站的角色。对于接收过程来说，网卡负责往RingBuffer写入收到的数据帧，ksoftirqd内核线程负责从中
取走处理。只要ksoftirqd工作足够快，RingBuffer就不会出现问题；但是如某一时刻，瞬间来了特别多的包，而ksoftirqd处理不过来，RingBuffer可能瞬间被
填满，后面再来的包，被网卡直接丢弃，不做任何处理

`ethtool -S ***(网卡名)` 查看丢包统计

2. 多队列网卡RSS调优

每个队列都有自己的中断号，有独立的中断号就可以独立向某个CPU核心发起硬中断请求。

在网卡支持多队列的服务器上，想提高内核收包的能力，直接简单加大队列数就可以，这比加大RingBuffer更为有用，因为加大RingBuffer只是提供更大的空间让
网络帧能继续排队，而加大队列数则能让包更早地被内核处理

在一般情况下，队列中断号和CPU之间的亲和性并不需要手工维护，有irqbalance服务来自动管理，其会根据系统中断负载的情况，自动维护和迁移各个中断的CPU亲和性，
以保持各个CPU之间的中断开销均衡。如果确实需要自己维护亲和性，那要先关掉irqbalance，然后再修改中断号对应的smp_affinity

3. 硬中断合并

CPU要做一件事情之前，要加载该进程的地址空间，装入进程代码，读取进程数据，各级别cache要慢慢热身，如果能够适当降低中断的频次，多攒几个包一起发出中断，对提升
CPU的整体工作效率有帮助

减少中断数量虽然能够使得Linux整体网络包吞吐更高，不过一些包的延迟也会增大，所以用的时候要适当注意


4. 软中断budget调整

```
sysctl -a |grep

net.core.netdev_budget = 300
```

其意思是ksoftirqd一次最多处理300个包，处理够了就会把CPU主动让出来，以便Linux上其他的任务得到处理，如果想提高内核处理网络包的效率，可以让ksoftirqd进程
多干一会儿网络包的接收，再让出CPU

/etc/sysctl.conf


5. 接收处理合并

硬中断合并是指的攒一堆数据包后再通知一次CPU，不过数据包仍然是分开的。LRO(Large Receive Offload)/GRO(Generic Receive Offload)还能将数据包合并后再往上层传递。

如果应用中是大文件的传输，大部分包是一段数据，不用LRO/GRO的话，会每次都将一个小包传达到协议栈函数中进行处理。开启后，内核或者网卡会进行包的合并，之后将一个大包传给
协议处理函数，这样CPU的效率也就提高了

LRO和GRO的区别是合并包的位置不同。LRO是在网卡上就把合并的事情做了，因此要求网卡硬件必须支持才行；而GRO是在内核源码中用软件的方式实现的，更加通用，不依赖硬件


发送过程优化

1. 控制数据包大小

在发送协议栈执行的过程中到了IP层如果要发送的数据大于MTU，会被分片。在分片的过程中多了一次内存拷贝，其次就是分片越多，在网络传输过程中出现丢包的风险就越大，
可以将尝试将数据大小控制在一个MTU内来极致地提高性能

2. 减少内存拷贝

目前减少内存拷贝主要有两种方法，分别是使用mmap和sendfile两个系统调用。

使用mmap，映射进来的这段地址空间的内存在用户态和内核态都是可以使用的，如果发送数据是mmap映射进来的数据，则内核直接就可以从地址空间中读取，这样就节约了
一次从内核态到用户态的拷贝过程，不过在mmap发送文件的方式里，系统调用的开销并没有减少，还是发生两次内核态和用户态的上下文切换。

如果只想将一个文件发送出去，而不关心它的内容，则可以调用另外一个做得更极致的系统调用---sendfile，在这个系统调用里，彻底把读文件和发送文件合并起来，
系统调用的开销又省了一次，再配合SG-DMA功能，可以直接从PageCache缓存区中DMA拷贝到网卡中，这样绝大部分的CPU拷贝操作就都省去了。

3. 推迟分片

4. 多队列网卡XPS调优

5. 使用eBPF绕开协议栈的本机网络IO

如果业务中涉及大量的本机网络IO可以考虑这个优化方案。

本机网络IO和跨机IO比较起来，确实是节省了驱动上的一些开销。发送数据不需要进RingBuffer的驱动队列，直接把skb传给接收协议栈（经过软中断），但是在内核的其他组件上，
工作量一点没少，系统调用、协议栈（传输层、网路层等）、设备子系统整个走了一遍，连“驱动”程序都走了(虽然对于回环设备来说这个驱动只是纯软件的虚拟出来的)。

如果想用本机网络IO，但是又不想频繁地在协议栈中绕来绕去，可以使用eBPF。使用eBPF的sockmap和sk redirect可以绕过TCP/IP协议栈，而被直接发送给接收端的socket。


内核与进程协作优化

1. 尽量少用recvfrom等进程阻塞的方式

2. 使用成熟的网络库

3. 使用Kernel-ByPass新技术

如DPDK


握手挥手过程优化

1. 配置充足的端口范围

2. 客户端最好不要使用bind

3. 小心连接队列溢出

4. 减少握手重试

5. 打开TFO(TCP Fast Open)

在客户端和服务端都支持该功能的前提下，客户端的第三次握手ack包就可以携带要发送给服务器的数据，这样就可以节约一个RTT的时间开销。

6. 保持充足的文件描述符上限

7. 如果请求频繁，请弃用短连接改用长连接

节约了握手开销

规避了队列满的问题

端口数不容易出问题

8. TIME_WAIT的优化

                                                                                                                      